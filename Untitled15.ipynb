{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4a1b18d",
   "metadata": {},
   "source": [
    "# STEPS FOR GENERATING TRANSCRIPTS INSIGHTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa397c0f",
   "metadata": {},
   "source": [
    "1. Preprocessing: Clean and preprocess the transcript data, including removing any irrelevant information, converting to lowercase, and removing punctuation and stop words.\n",
    "\n",
    "2. Sentence segmentation: Divide the transcript data into sentences using Natural Language Processing (NLP) techniques such as sentence tokenization.\n",
    "\n",
    "3. Topic modeling: Use NLP techniques such as Latent Dirichlet Allocation (LDA) or Non-negative Matrix Factorization (NMF) to identify the topics discussed in the meeting.\n",
    "\n",
    "4. Keyword extraction: Extract important keywords or phrases from the transcript data that summarize the main points discussed in each topic.\n",
    "\n",
    "5. Sentiment analysis: Analyze the sentiment of each sentence in the transcript data to determine the overall tone of the meeting.\n",
    "\n",
    "6. Action item extraction: Extract any action items or decisions made during the meeting, such as tasks assigned to specific individuals or deadlines.\n",
    "\n",
    "7. Organizing and summarizing: Organize the extracted information into a concise, readable format, such as bullet points or a table, and summarize the main points discussed in each topic.\n",
    "\n",
    "8. Writing the minutes: Write the minutes of the meeting, including the date, time, attendees, topics discussed, actions taken, and any other relevant information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7458adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\p_adi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Important Libraries\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import warnings\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0644dda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_text = ['PREOPERATIVE DIAGNOSIS: , Morbid obesity.,POSTOPERATIVE DIAGNOSIS:  ,Morbid obesity.,PROCEDURE: , Laparoscopic antecolic antegastric Roux-en-Y gastric bypass with EEA anastomosis.,ANESTHESIA: , General with endotracheal intubation.,INDICATION FOR PROCEDURE: , This is a 30-year-old female, who has been overweight for many years.  She has tried many different diets, but is unsuccessful.  She has been to our Bariatric Surgery Seminar, received some handouts, and signed the consent.  The risks and benefits of the procedure have been explained to the patient.,PROCEDURE IN DETAIL:  ,The patient was taken to the operating room and placed supine on the operating room table.  All pressure points were carefully padded.  She was given general anesthesia with endotracheal intubation.  SCD stockings were placed on both legs.  Foley catheter was placed for bladder decompression.  The abdomen was then prepped and draped in standard sterile surgical fashion.  Marcaine was then injected through umbilicus.  A small incision was made.  A Veress needle was introduced into the abdomen.  CO2 insufflation was done to a maximum pressure of 15 mmHg.  A 12-mm VersaStep port was placed through the umbilicus.  I then placed a 5-mm port just anterior to the midaxillary line and just subcostal on the right side.  I placed another 5-mm port in the midclavicular line just subcostal on the right side, a few centimeters below and medial to that, I placed a 12-mm VersaStep port.  On the left side, just anterior to the midaxillary line and just subcostal, I placed a 5-mm port.  A few centimeters below and medial to that, I placed a 15-mm port.  I began by lifting up the omentum and identifying the transverse colon and lifting that up and thereby identifying my ligament of Treitz.  I ran the small bowel down approximately 40 cm and divided the small bowel with a white load GIA stapler.  I then divided the mesentery all the way down to the base of the mesentery with a LigaSure device.  I then ran the distal bowel down, approximately 100 cm, and at 100 cm, I made a hole at the antimesenteric portion of the Roux limb and a hole in the antimesenteric portion of the duodenogastric limb, and I passed a 45 white load stapler and fired a stapler creating a side-to-side anastomosis.  I reapproximated the edges of the defect.  I lifted it up and stapled across it with another white load stapler.  I then closed the mesenteric defect with interrupted Surgidac sutures.  I divided the omentum all the way down to the colon in order to create a passageway for my small bowel to go antecolic.  I then put the patient in reverse Trendelenburg.  I placed a liver retractor, identified, and dissected the angle of His.  I then dissected on the lesser curve, approximately 2.5 cm below the gastroesophageal junction, and got into a lesser space.  I fired transversely across the stomach with a 45 blue load stapler.  I then used two fires of the 60 blue load with SeamGuard to go up into my angle of His, thereby creating my gastric pouch.  I then made a hole at the base of the gastric pouch and had Anesthesia remove the bougie and place the OG tube connected to the anvil.  I pulled the anvil into place, and I then opened up my 15-mm port site and passed my EEA stapler.  I passed that in the end of my Roux limb and had the spike come out antimesenteric.  I joined the spike with the anvil and fired a stapler creating an end-to-side anastomosis, then divided across the redundant portion of my Roux limb with a white load GI stapler, and removed it with an Endocatch bag.  I put some additional 2-0 Vicryl sutures in the anastomosis for further security.  I then placed a bowel clamp across the bowel.  I went above and passed an EGD scope into the mouth down to the esophagus and into the gastric pouch.  I distended gastric pouch with air.  There was no air leak seen.  I could pass the scope easily through the anastomosis.  There was no bleeding seen through the scope.  We closed the 15-mm port site with interrupted 0 Vicryl suture utilizing Carter-Thomason.  I copiously irrigated out that incision with about 2 L of saline.  I then closed the skin of all incisions with running Monocryl.  Sponge, instrument, and needle counts were correct at the end of the case.  The patient tolerated the procedure well without any complications.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c67fd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_text = \" \".join(transcript_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81508650",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def preprocess_transcript(transcript_text):\n",
    "    # Remove special characters and digits\n",
    "    transcript_text = re.sub(\"(\\\\d|\\\\W)+\", \" \", transcript_text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    transcript_text = transcript_text.lower()\n",
    "    \n",
    "    # Tokenize the words\n",
    "    words = word_tokenize(transcript_text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    words1 = \" \".join(words)\n",
    "    \n",
    "    return words1\n",
    "\n",
    "def extract_topics(transcript_text, num_topics=5, model=\"nmf\"):\n",
    "    preprocessed_transcript = preprocess_transcript(transcript_text)\n",
    "    \n",
    "    # Vectorize the transcript text\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform([preprocessed_transcript])\n",
    "    \n",
    "    if model == \"nmf\":\n",
    "        # Use NMF to extract topics\n",
    "        nmf = NMF(n_components=num_topics, random_state=1)\n",
    "        W = nmf.fit_transform(X)\n",
    "        H = nmf.components_\n",
    "    else:\n",
    "        # Use LDA to extract topics\n",
    "        lda = LatentDirichletAllocation(n_components=num_topics, random_state=1)\n",
    "        W = lda.fit_transform(X)\n",
    "        H = lda.components_\n",
    "    \n",
    "    # Get the feature names and topic words\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    topic_words = [\n",
    "        [\n",
    "            feature_names[i]\n",
    "            for i in topic.argsort()[:-10 - 1 :-1]\n",
    "        ]\n",
    "        for topic in H\n",
    "    ]\n",
    "    \n",
    "    return topic_words\n",
    "\n",
    "def extract_keywords(transcript_text, num_keywords=5):\n",
    "    preprocessed_transcript = preprocess_transcript(transcript_text)\n",
    "    \n",
    "    # Vectorize the transcript text\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform([preprocessed_transcript])\n",
    "    \n",
    "    # Get the feature names and keyword counts\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    keyword_counts = X.toarray().sum(axis=0)\n",
    "    \n",
    "    # Get the top keywords\n",
    "    keywords = [\n",
    "        feature_names[i]\n",
    "        for i in keyword_counts.argsort()[:-num_keywords - 1 :-1]\n",
    "    ]\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "874d8f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'preoperative diagnosis morbid obesity postoperative diagnosis morbid obesity procedure laparoscopic antecolic antegastric roux en gastric bypass eea anastomosis anesthesia general endotracheal intubation indication procedure year old female overweight many years tried many different diets unsuccessful bariatric surgery seminar received handouts signed consent risks benefits procedure explained patient procedure detail patient taken operating room placed supine operating room table pressure points carefully padded given general anesthesia endotracheal intubation scd stockings placed legs foley catheter placed bladder decompression abdomen prepped draped standard sterile surgical fashion marcaine injected umbilicus small incision made veress needle introduced abdomen co insufflation done maximum pressure mmhg mm versastep port placed umbilicus placed mm port anterior midaxillary line subcostal right side placed another mm port midclavicular line subcostal right side centimeters medial placed mm versastep port left side anterior midaxillary line subcostal placed mm port centimeters medial placed mm port began lifting omentum identifying transverse colon lifting thereby identifying ligament treitz ran small bowel approximately cm divided small bowel white load gia stapler divided mesentery way base mesentery ligasure device ran distal bowel approximately cm cm made hole antimesenteric portion roux limb hole antimesenteric portion duodenogastric limb passed white load stapler fired stapler creating side side anastomosis reapproximated edges defect lifted stapled across another white load stapler closed mesenteric defect interrupted surgidac sutures divided omentum way colon order create passageway small bowel go antecolic put patient reverse trendelenburg placed liver retractor identified dissected angle dissected lesser curve approximately cm gastroesophageal junction got lesser space fired transversely across stomach blue load stapler used two fires blue load seamguard go angle thereby creating gastric pouch made hole base gastric pouch anesthesia remove bougie place og tube connected anvil pulled anvil place opened mm port site passed eea stapler passed end roux limb spike come antimesenteric joined spike anvil fired stapler creating end side anastomosis divided across redundant portion roux limb white load gi stapler removed endocatch bag put additional vicryl sutures anastomosis security placed bowel clamp across bowel went passed egd scope mouth esophagus gastric pouch distended gastric pouch air air leak seen could pass scope easily anastomosis bleeding seen scope closed mm port site interrupted vicryl suture utilizing carter thomason copiously irrigated incision l saline closed skin incisions running monocryl sponge instrument needle counts correct end case patient tolerated procedure well without complications'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1 = preprocess_transcript(transcript_text)\n",
    "word1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23dc1449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\p_adi\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:117: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(res * 2)\n",
      "C:\\Users\\p_adi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['placed',\n",
       "  'port',\n",
       "  'stapler',\n",
       "  'mm',\n",
       "  'side',\n",
       "  'procedure',\n",
       "  'load',\n",
       "  'gastric',\n",
       "  'anastomosis',\n",
       "  'bowel'],\n",
       " ['cm',\n",
       "  'anesthesia',\n",
       "  'mesentery',\n",
       "  'spike',\n",
       "  'approximately',\n",
       "  'seen',\n",
       "  'abdomen',\n",
       "  'go',\n",
       "  'divided',\n",
       "  'medial'],\n",
       " ['pressure',\n",
       "  'umbilicus',\n",
       "  'junction',\n",
       "  'pouch',\n",
       "  'co',\n",
       "  'ran',\n",
       "  'defect',\n",
       "  'lifting',\n",
       "  'across',\n",
       "  'morbid'],\n",
       " ['fired',\n",
       "  'across',\n",
       "  'bowel',\n",
       "  'limb',\n",
       "  'omentum',\n",
       "  'midaxillary',\n",
       "  'anterior',\n",
       "  'lifting',\n",
       "  'bypass',\n",
       "  'stockings'],\n",
       " ['anvil',\n",
       "  'irrigated',\n",
       "  'redundant',\n",
       "  'mm',\n",
       "  'sutures',\n",
       "  'interrupted',\n",
       "  'medial',\n",
       "  'reverse',\n",
       "  'surgical',\n",
       "  'preoperative']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_words = extract_topics(word1)\n",
    "t_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40b32fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['placed', 'mm', 'port', 'stapler', 'load']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_word = extract_keywords(word1)\n",
    "k_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46f74812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'neg': 0.082, 'neu': 0.866, 'pos': 0.052, 'compound': -0.9231}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_words = extract_sentiment(word1)\n",
    "s_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11cc8e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_word = extract_action_items(word1)\n",
    "e_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ac5a8f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'topic': ['placed',\n",
       "   'port',\n",
       "   'stapler',\n",
       "   'mm',\n",
       "   'side',\n",
       "   'procedure',\n",
       "   'load',\n",
       "   'gastric',\n",
       "   'anastomosis',\n",
       "   'bowel'],\n",
       "  'sentiment': 0.019224999999999992,\n",
       "  'action_items': []}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = summarize_topics(t_words, s_words, e_word)\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ca52c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aefe2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312caa62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d32ac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentiment(transcript_text):\n",
    "    # Initialize the sentiment analyzer\n",
    "    sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    # Tokenize the transcript into sentences\n",
    "    sentences = sent_tokenize(transcript_text)\n",
    "    \n",
    "    # Get the sentiment scores for each sentence\n",
    "    sentiments = [sentiment_analyzer.polarity_scores(sentence) for sentence in sentences]\n",
    "    \n",
    "    return sentiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a212d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_action_items(transcript_text):\n",
    "    # Regular expression pattern to match action items\n",
    "    pattern = re.compile(r\"^\\s*(\\w+)\\s*will\\s+(.+)\\s+by\\s+(.+)$\", re.MULTILINE)\n",
    "    \n",
    "    # Extract the action items\n",
    "    action_items = re.findall(pattern, transcript_text)\n",
    "    \n",
    "    return action_items\n",
    "\n",
    "def summarize_topics(topics, sentiments, action_items):\n",
    "    # Initialize a list to store the summarized information\n",
    "    summary = []\n",
    "    \n",
    "    # Loop through the topics\n",
    "    for topic, sentiment in zip(topics, sentiments):\n",
    "        # Get the average sentiment for the topic\n",
    "        avg_sentiment = sum(sentiment.values()) / len(sentiment.values())\n",
    "        \n",
    "        # Filter the action items for the topic\n",
    "        topic_actions = [item for item in action_items if topic in item[1]]\n",
    "        \n",
    "        # Summarize the topic information\n",
    "        topic_summary = {\n",
    "            \"topic\": topic,\n",
    "            \"sentiment\": avg_sentiment,\n",
    "            \"action_items\": topic_actions\n",
    "        }\n",
    "        \n",
    "        summary.append(topic_summary)\n",
    "    \n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a648c9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cbb6a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d1182e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97ed9c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e6b2fd6f7e4827bd7b949df0234645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\p_adi\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:129: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\p_adi\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c068d897c84fe8a019f1ea24aa4999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3257912573024d218e3007e2f9ec4edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c51135a84e2a424eaee70c2b49cfb69b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "192d3128e046452bb2533ea053036bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load the BERT model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def extract_sentences(transcript_text):\n",
    "    # Split the transcript text into sentences\n",
    "    sentences = re.split(r\"(?<=[.!?])\\s+\", transcript_text)\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def predict_sentiment(sentences):\n",
    "    # Encode the sentences with BERT\n",
    "    inputs = tokenizer.batch_encode_plus(sentences, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Pass the encoded sentences through the BERT model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "        \n",
    "    # Get the sentiment scores for each sentence\n",
    "    sentiment_scores = torch.sigmoid(outputs[0]).mean(dim=1)\n",
    "    \n",
    "    return sentiment_scores\n",
    "\n",
    "def extract_action_items(transcript_text):\n",
    "    # Regular expression pattern to match action items\n",
    "    pattern = re.compile(r\"^\\s*(\\w+)\\s*will\\s+(.+)\\s+by\\s+(.+)$\", re.MULTILINE)\n",
    "    \n",
    "    # Extract the action items\n",
    "    action_items = re.findall(pattern, transcript_text)\n",
    "    \n",
    "    return action_items\n",
    "\n",
    "def summarize_topics(sentences, sentiment_scores, action_items):\n",
    "    # Initialize a list to store the summarized information\n",
    "    summary = []\n",
    "    \n",
    "    # Loop through the sentences and sentiment scores\n",
    "    for sentence, sentiment_score in zip(sentences, sentiment_scores):\n",
    "        # Filter the action items for the sentence\n",
    "        sentence_actions = [item for item in action_items if sentence in item[1]]\n",
    "        \n",
    "        # Summarize the sentence information\n",
    "        sentence_summary = {\n",
    "            \"sentence\": sentence,\n",
    "            \"sentiment_score\": sentiment_score,\n",
    "            \"action_items\": sentence_actions\n",
    "        }\n",
    "        \n",
    "        summary.append(sentence_summary)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def write_minutes(summary, attendees, date, time):\n",
    "    # Initialize the minutes text\n",
    "    minutes = \"Minutes of the Meeting\\n\\n\"\n",
    "    \n",
    "    # Add the date, time, and attendees\n",
    "    minutes += \"Date: {}\\n\".format(date)\n",
    "    minutes += \"Time: {}\\n\".format(time)\n",
    "    minutes += \"Attendees: {}\\n\\n\".format(\", \".join(attendees))\n",
    "    \n",
    "    # Loop through the summarized sentences\n",
    "    for sentence_summary in summary:\n",
    "        # Add the sentence\n",
    "        minutes += \"Sentence: {}\\n\".format(sentence_summary[\"sentence\"])\n",
    "        \n",
    "        # Add the sentiment score for the sentence\n",
    "        minutes += \"Sentiment Score: {:.2f}\\n\".format(sentence_summary[\"sentiment_score\"])\n",
    "        \n",
    "        # Add the action items for the\n",
    "        if topic_summary[\"action_items\"]:\n",
    "            minutes += \"Action Items:\\n\"\n",
    "            for item in topic_summary[\"action_items\"]:\n",
    "                minutes += \"- {} will {} by {}\\n\".format(*item)\n",
    "        minutes += \"\\n\"\n",
    "    \n",
    "    return minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3beead94",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to Tensor.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 31>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Call the main function\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 32\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m summary \u001b[38;5;241m=\u001b[39m summarize_topics(sentences, sentiment_scores, action_items)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Write the minutes of the meeting\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m minutes \u001b[38;5;241m=\u001b[39m \u001b[43mwrite_minutes\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattendees\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Print the minutes\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(minutes)\n",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36mwrite_minutes\u001b[1;34m(summary, attendees, date, time)\u001b[0m\n\u001b[0;32m     69\u001b[0m minutes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentence: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(sentence_summary[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Add the sentiment score for the sentence\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m minutes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSentiment Score: \u001b[39;49m\u001b[38;5;132;43;01m{:.2f}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_summary\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentiment_score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Add the action items for the\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m topic_summary[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction_items\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:660\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[1;34m(self, format_spec)\u001b[0m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta:\n\u001b[0;32m    659\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[1;32m--> 660\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__format__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_spec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported format string passed to Tensor.__format__"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # The transcript text\n",
    "    transcript_text = \"John: Good morning everyone.\\n\\nJane: Hi John.\\n\\nJohn: Before we start, does anyone have any items to add to the agenda?\\n\\nJane: I would like to propose that we discuss our sales strategy for the next quarter.\\n\\nJohn: Great, let's add that to the agenda.\\n\\nJane: Also, I would like to follow up on the action item from the last meeting. I was supposed to create a report on our expenses and I will have that ready by tomorrow.\\n\\nJohn: Thanks, Jane. Can we add that to the agenda as well?\\n\\nJane: Sure, no problem.\\n\\nJohn: Alright, let's get started. First on the agenda, we have our sales strategy for the next quarter. Does anyone have any ideas on how we can improve our sales?\\n\\nJane: I was thinking that we could reach out to our current customers and offer them a loyalty discount.\\n\\nJohn: That's a great idea. Can you add that to our action items and have it ready for the next meeting?\\n\\nJane: Sure, I will have that done by next week.\\n\\nJohn: Alright, let's move on to the next item on the agenda. Does anyone have any questions or concerns regarding the expenses report?\\n\\nJane: No, I don't think so.\\n\\nJohn: Alright, then let's wrap up the meeting. Does anyone have any last minute items to add?\\n\\nJane: No, that's all from me.\\n\\nJohn: Great, then let's adjourn the meeting. Have a good day everyone.\\n\\nJane: You too.\\n\"\n",
    "    \n",
    "    # The attendees\n",
    "    attendees = [\"John\", \"Jane\"]\n",
    "    \n",
    "    # The date and time of the meeting\n",
    "    date = \"09-Feb-2023\"\n",
    "    time = \"10:00 AM\"\n",
    "    \n",
    "    # Extract the sentences from the transcript text\n",
    "    sentences = extract_sentences(transcript_text)\n",
    "    \n",
    "    # Predict the sentiment scores for the sentences\n",
    "    sentiment_scores = predict_sentiment(sentences)\n",
    "    \n",
    "    # Extract the action items from the transcript text\n",
    "    action_items = extract_action_items(transcript_text)\n",
    "    \n",
    "    # Summarize the sentences and sentiment scores\n",
    "    summary = summarize_topics(sentences, sentiment_scores, action_items)\n",
    "    \n",
    "    # Write the minutes of the meeting\n",
    "    minutes = write_minutes(summary, attendees, date, time)\n",
    "    \n",
    "    # Print the minutes\n",
    "    print(minutes)\n",
    "\n",
    "# Call the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6b6f74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def sentiment_analysis(transcripts):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    transcript = \" \".join(transcripts)\n",
    "    doc = nlp(transcript)\n",
    "    \n",
    "    sentiments = []\n",
    "    for sent in doc.sents:\n",
    "        sentiments.append(sent._.sentiment)\n",
    "    \n",
    "    average_sentiment = sum(sentiments) / len(sentiments)\n",
    "    return {'average_sentiment': average_sentiment}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16335eca",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "[E046] Can't retrieve unregistered extension attribute 'sentiment'. Did you forget to call the `set_extension` method?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m transcripts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a sample meeting transcript. The meeting was productive and all attendees were positive about the outcome.\u001b[39m\u001b[38;5;124m\"\u001b[39m,               \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is another meeting transcript. The meeting was unproductive and attendees were negative about the outcome.\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      2\u001b[0m transcripts \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(transcripts)\n\u001b[1;32m----> 3\u001b[0m sentiment \u001b[38;5;241m=\u001b[39m \u001b[43msentiment_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranscripts\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36msentiment_analysis\u001b[1;34m(transcripts)\u001b[0m\n\u001b[0;32m      9\u001b[0m sentiments \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msents:\n\u001b[1;32m---> 11\u001b[0m     sentiments\u001b[38;5;241m.\u001b[39mappend(\u001b[43msent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentiment\u001b[49m)\n\u001b[0;32m     13\u001b[0m average_sentiment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(sentiments) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(sentiments)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_sentiment\u001b[39m\u001b[38;5;124m'\u001b[39m: average_sentiment}\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\tokens\\underscore.py:47\u001b[0m, in \u001b[0;36mUnderscore.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extensions:\n\u001b[1;32m---> 47\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE046\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m     48\u001b[0m     default, method, getter, setter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extensions[name]\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m getter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: [E046] Can't retrieve unregistered extension attribute 'sentiment'. Did you forget to call the `set_extension` method?"
     ]
    }
   ],
   "source": [
    "transcripts = [\"This is a sample meeting transcript. The meeting was productive and all attendees were positive about the outcome.\",               \"This is another meeting transcript. The meeting was unproductive and attendees were negative about the outcome.\"]\n",
    "transcripts = \" \".join(transcripts)\n",
    "sentiment = sentiment_analysis(transcripts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f250ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Token\n",
    "\n",
    "Token.set_extension(\"sentiment\", default=0, force=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1eb72b0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "[E046] Can't retrieve unregistered extension attribute 'sentiment'. Did you forget to call the `set_extension` method?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a positive sentence.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m sentiment \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentiment\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(sentiment)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\tokens\\underscore.py:50\u001b[0m, in \u001b[0;36mUnderscore.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     48\u001b[0m default, method, getter, setter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extensions[name]\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m getter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m     method_partial \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(method, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj)\n",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36msentiment_score\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m     10\u001b[0m sentiments \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msents:\n\u001b[1;32m---> 12\u001b[0m     sentiments\u001b[38;5;241m.\u001b[39mappend(\u001b[43msent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentiment\u001b[49m)\n\u001b[0;32m     14\u001b[0m average_sentiment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(sentiments) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(sentiments)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# and return a sentiment score for the given doc\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\tokens\\underscore.py:47\u001b[0m, in \u001b[0;36mUnderscore.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extensions:\n\u001b[1;32m---> 47\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE046\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m     48\u001b[0m     default, method, getter, setter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extensions[name]\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m getter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: [E046] Can't retrieve unregistered extension attribute 'sentiment'. Did you forget to call the `set_extension` method?"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "def sentiment_score(doc):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    transcript = \" \".join(transcripts)\n",
    "    doc = nlp(transcript)\n",
    "    \n",
    "    sentiments = []\n",
    "    for sent in doc.sents:\n",
    "        sentiments.append(sent._.sentiment)\n",
    "    \n",
    "    average_sentiment = sum(sentiments) / len(sentiments)\n",
    "    # and return a sentiment score for the given doc\n",
    "    return sentiment_score\n",
    "\n",
    "Doc.set_extension(\"sentiment\", getter=sentiment_score, force=True)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"This is a positive sentence.\")\n",
    "sentiment = doc._.sentiment\n",
    "print(sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bfc0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
