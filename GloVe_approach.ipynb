{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06ea7284",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'pad_sequences' from 'keras.preprocessing.sequence' (C:\\Users\\p_adi\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\sequence.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeyedvectors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KeyedVectors\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'pad_sequences' from 'keras.preprocessing.sequence' (C:\\Users\\p_adi\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\sequence.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models.keyedvectors import KeyedVectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288e3245",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc1b0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GloVe model\n",
    "glove_model = KeyedVectors.load_word2vec_format('glove.6B.100d.txt', binary=False)\n",
    "\n",
    "# Tokenize the words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['correct_words'])\n",
    "\n",
    "# convert words to numerical inputs using GloVe word embeddings\n",
    "correct_words = pad_sequences(tokenizer.texts_to_sequences(data['correct_words']))\n",
    "misspelled_words = pad_sequences(tokenizer.texts_to_sequences(data['misspelled_words']))\n",
    "\n",
    "# Create the embedding matrix\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index)+1, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in glove_model:\n",
    "        embedding_matrix[i] = glove_model[word]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf48184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense, Input, TimeDistributed\n",
    "from keras.models import Model\n",
    "\n",
    "# Define the input layer\n",
    "input_layer = Input(shape=(None,))\n",
    "\n",
    "# Define the embedding layer\n",
    "embedding_layer = Embedding(len(tokenizer.word_index)+1, 100, input_length=None,weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "\n",
    "# Define the LSTM layer\n",
    "lstm_layer = LSTM(100, return_sequences=True)(embedding_layer)\n",
    "\n",
    "# Define the output layer\n",
    "output_layer = TimeDistributed(Dense(len(tokenizer.word_index)+1, activation='softmax'))(lstm_layer)\n",
    "\n",
    "# Create the model\n",
    "model = Model(input_layer, output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb91ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "model.fit(correct_words, misspelled_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cd8c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the misspelled words\n",
    "misspelled_words = pad_sequences(tokenizer.texts_to_sequences([\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4b175c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deac9368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d235303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ed6f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7121a23e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60112a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "# load the GloVe embeddings\n",
    "embeddings_index = {}\n",
    "with open('glove.6B.100d.txt') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "# Tokenize the correct and misspelled words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(list(data['correct_words'].values) + list(data['misspelled_words'].values))\n",
    "\n",
    "# Convert the words to sequences of integers\n",
    "correct_sequences = tokenizer.texts_to_sequences(data['correct_words'].values)\n",
    "misspelled_sequences = tokenizer.texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d4f379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding matrix with the GloVe embeddings\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Use the embedding matrix as the weights for the embedding layer\n",
    "embedding_layer = Embedding(len(tokenizer.word_index) + 1,\n",
    "                            100,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n",
    "encoder_embedding = embedding_layer(encoder_inputs)\n",
    "decoder_embedding = embedding_layer(decoder_inputs)\n",
    "\n",
    "# Define and compile the model\n",
    "model = Model([encoder_inputs, decoder_inputs], outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Fit the model on the data\n",
    "model.fit([correct_sequences, misspelled_sequences], correct_sequences,\n",
    "          batch_size=32, epochs=100, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804c37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on new misspelled words\n",
    "new_misspelled_words = [\"appel\", \"bananaa\", \"orangge\"]\n",
    "new_misspelled_sequences = tokenizer.texts_to_sequences(new_misspelled_words)\n",
    "new_misspelled_sequences = pad_sequences(new_misspelled_sequences, maxlen=max_length)\n",
    "\n",
    "# Define the encoder and decoder models for making predictions\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "decoder_state_input_h = Input(shape=(100,))\n",
    "decoder_state_input_c = Input(shape=(100,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder(decoder_embedding, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Use the encoder and decoder models to make predictions\n",
    "states_value = encoder_model.predict(new_misspelled_sequences)\n",
    "predicted_words = []\n",
    "for i in range(len(new_misspelled_sequences)):\n",
    "    decoded_sentence = \"\"\n",
    "    target_seq = np.zeros((1, max_length))\n",
    "    target_seq[0, 0] = 1\n",
    "    stop_condition = False\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = tokenizer.index_word[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_word\n",
    "        if (sampled_word == \"end\" or len(decoded_sentence.split()) >= max_length):\n",
    "            stop_condition = True\n",
    "        target_seq = np.zeros((1, max_length))\n",
    "        target_seq[0, 0] = sampled_token_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd19e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6e4bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6c5896d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct words: ['pear']\n",
      "Incorrect words: ['carrrot', 'potatoe', 'womman', 'pumbkin', 'morpidopesity', 'metholienate', 'chelesteral', 'hyperdipillemia', 'hyperlifichesmia', 'folliaded', 'Dratactive', 'towastatin', 'chelesteral', 'patision', 'Oregozale', 'Vigran', 'Farxija']\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "# list of words to check\n",
    "words = ['carrrot', 'pear', 'potatoe', 'womman', 'pumbkin', 'morpidopesity', 'metholienate', 'chelesteral', 'hyperdipillemia', 'hyperlifichesmia', 'folliaded',\n",
    "             'Dratactive', 'towastatin', 'chelesteral', 'patision', 'Oregozale', 'Vigran', 'Farxija']\n",
    "\n",
    "correct_words = []\n",
    "incorrect_words = []\n",
    "\n",
    "for word in words:\n",
    "    if spell.correction(word) == word:\n",
    "        correct_words.append(word)\n",
    "    else:\n",
    "        incorrect_words.append(word)\n",
    "\n",
    "print(\"Correct words:\", correct_words)\n",
    "print(\"Incorrect words:\", incorrect_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8855b060",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SpellChecker' object has no attribute 'batch_correction'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# list of words to check\u001b[39;00m\n\u001b[0;32m      6\u001b[0m words \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcarrrot\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpotatoe\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwomman\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpumbkin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmorpidopesity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetholienate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchelesteral\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhyperdipillemia\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhyperlifichesmia\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfolliaded\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDratactive\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtowastatin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchelesteral\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatision\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOregozale\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVigran\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFarxija\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 9\u001b[0m correct_words, incorrect_words \u001b[38;5;241m=\u001b[39m \u001b[43mspell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_correction\u001b[49m(words)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrect words:\u001b[39m\u001b[38;5;124m\"\u001b[39m, correct_words)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect words:\u001b[39m\u001b[38;5;124m\"\u001b[39m, incorrect_words)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SpellChecker' object has no attribute 'batch_correction'"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "# list of words to check\n",
    "words = ['carrrot', 'pear', 'potatoe', 'womman', 'pumbkin', 'morpidopesity', 'metholienate', 'chelesteral', 'hyperdipillemia', 'hyperlifichesmia', 'folliaded',\n",
    "             'Dratactive', 'towastatin', 'chelesteral', 'patision', 'Oregozale', 'Vigran', 'Farxija']\n",
    "\n",
    "correct_words, incorrect_words = spell.batch_correction(words)\n",
    "\n",
    "print(\"Correct words:\", correct_words)\n",
    "print(\"Incorrect words:\", incorrect_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e47cd6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct words: []\n",
      "Incorrect words: ['morpidopesity', 'metholienate', 'chelesteral', 'hyperdipillemia', 'hyperlifichesmia', 'folliaded', 'Dratactive', 'towastatin', 'chelesteral', 'patision', 'Oregozale', 'Vigran', 'Farxija']\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "# list of words to check\n",
    "words = ['morpidopesity', 'metholienate', 'chelesteral', 'hyperdipillemia', 'hyperlifichesmia', 'folliaded',\n",
    "             'Dratactive', 'towastatin', 'chelesteral', 'patision', 'Oregozale', 'Vigran', 'Farxija']\n",
    "\n",
    "correct_words = [word for word in words if spell.correction(word) == word]\n",
    "incorrect_words = [word for word in words if spell.correction(word) != word]\n",
    "\n",
    "print(\"Correct words:\", correct_words)\n",
    "print(\"Incorrect words:\", incorrect_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6c256c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "data = pd.read_csv('sample_data.csv')\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "def check_spelling(text):\n",
    "    words = text.split()\n",
    "    correct_words = {word: 'c' for word in words if spell.correction(word) == word}\n",
    "    incorrect_words = {word: 'inc' for word in words if spell.correction(word) != word}\n",
    "    return {**correct_words, **incorrect_words}\n",
    "\n",
    "data['coutput_column'] = data['input_text'].apply(lambda x: check_spelling(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30e617e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3], [0, 0, 4, 5], [6, 7, 8, 9]]\n"
     ]
    }
   ],
   "source": [
    "# List of sequences\n",
    "sequences = [[1, 2, 3], [4, 5], [6, 7, 8, 9]]\n",
    "\n",
    "# Maximum length of the sequences\n",
    "max_len = 4\n",
    "\n",
    "# Initialize a list to store the padded sequences\n",
    "padded_sequences = []\n",
    "\n",
    "# Iterate through each sequence\n",
    "for sequence in sequences:\n",
    "    # Calculate the number of padding elements needed\n",
    "    num_padding = max_len - len(sequence)\n",
    "    \n",
    "    # Add the padding elements to the beginning of the sequence\n",
    "    new_sequence = [0]*num_padding + sequence\n",
    "    \n",
    "    # Append the padded sequence to the list\n",
    "    padded_sequences.append(new_sequence)\n",
    "\n",
    "print(padded_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83405680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum length of the sequences\n",
    "max_len = 4\n",
    "\n",
    "# Initialize a list to store the padded sequences\n",
    "padded_sequences = []\n",
    "\n",
    "# Iterate through each sequence\n",
    "for sequence in sequences:\n",
    "    # Calculate the number of padding elements needed\n",
    "    num_padding = max_len - len(sequence)\n",
    "    \n",
    "    # Add the padding elements to the end of the sequence\n",
    "    new_sequence = sequence + [0]*num_padding\n",
    "    \n",
    "    # Append the padded sequence to the list\n",
    "    padded_sequences.append(new_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3c2fbfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 0], [4, 5, 0, 0], [6, 7, 8, 9]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ebd3a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
