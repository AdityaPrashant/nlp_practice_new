{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cc807b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b1fb8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"C:/Users/p_adi/OneDrive/Desktop/output.xlsx\")\n",
    "df.columns = ['word', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "989ab95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>postural hypotension</td>\n",
       "      <td>disease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>parkinson ' s disease</td>\n",
       "      <td>disease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>systolic orthostatic hypotension</td>\n",
       "      <td>disease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>orthostatic hypotension</td>\n",
       "      <td>disease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reduced the supine systolic and diastolic bloo...</td>\n",
       "      <td>disease</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                word    label\n",
       "0                               postural hypotension  disease\n",
       "1                              parkinson ' s disease  disease\n",
       "2                   systolic orthostatic hypotension  disease\n",
       "3                            orthostatic hypotension  disease\n",
       "4  reduced the supine systolic and diastolic bloo...  disease"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbd40892",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4f533f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = list(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "593df6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    43791\n",
       "True     19895\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc9a3754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    41292\n",
       "True     22394\n",
       "Name: word, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = df['word'].duplicated()\n",
    "b.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a6aff13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43791, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_updated = df.drop_duplicates()\n",
    "df_updated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fbe5c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "other      27492\n",
       "drug       24357\n",
       "disease    11837\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "728750cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated.to_csv('output_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f50027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f7cabf8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E001] No component '<spacy.pipeline.ner.EntityRecognizer object at 0x00000238B42B4890>' found in pipeline. Available names: ['tok2vec', 'tagger', 'parser', 'senter', 'attribute_ruler', 'lemmatizer', 'ner']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [56]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m nlp\u001b[38;5;241m.\u001b[39mpipe_names:\n\u001b[0;32m      9\u001b[0m     ner \u001b[38;5;241m=\u001b[39m nlp\u001b[38;5;241m.\u001b[39mget_pipe(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m     \u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove_pipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Create a new NER component and add it to the pipeline\u001b[39;00m\n\u001b[0;32m     13\u001b[0m ner \u001b[38;5;241m=\u001b[39m nlp\u001b[38;5;241m.\u001b[39madd_pipe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py:950\u001b[0m, in \u001b[0;36mLanguage.remove_pipe\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    942\u001b[0m \u001b[38;5;124;03m\"\"\"Remove a component from the pipeline.\u001b[39;00m\n\u001b[0;32m    943\u001b[0m \n\u001b[0;32m    944\u001b[0m \u001b[38;5;124;03mname (str): Name of the component to remove.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[38;5;124;03mDOCS: https://spacy.io/api/language#remove_pipe\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    949\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponent_names:\n\u001b[1;32m--> 950\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE001\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, opts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponent_names))\n\u001b[0;32m    951\u001b[0m removed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_components\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponent_names\u001b[38;5;241m.\u001b[39mindex(name))\n\u001b[0;32m    952\u001b[0m \u001b[38;5;66;03m# We're only removing the component itself from the metas/configs here\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;66;03m# because factory may be used for something else\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: [E001] No component '<spacy.pipeline.ner.EntityRecognizer object at 0x00000238B42B4890>' found in pipeline. Available names: ['tok2vec', 'tagger', 'parser', 'senter', 'attribute_ruler', 'lemmatizer', 'ner']"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Remove the pre-trained NER component from the pipeline\n",
    "if 'ner' in nlp.pipe_names:\n",
    "    ner = nlp.get_pipe('ner')\n",
    "    nlp.remove_pipe(ner)\n",
    "\n",
    "# Create a new NER component and add it to the pipeline\n",
    "ner = nlp.add_pipe(\"ner\")\n",
    "\n",
    "#Add the labels\n",
    "for label in [\"Disease\", \"Drug\", \"Others\"]:\n",
    "    ner.add_label(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c23c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataframe\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Create the training examples\n",
    "train_data = []\n",
    "for i in range(len(df)):\n",
    "    text = df.iloc[i][\"word\"]\n",
    "    label = df.iloc[i][\"label\"]\n",
    "    train_data.append((text, {\"entities\": [(0, len(text), label)]}))\n",
    "\n",
    "# Start the training\n",
    "for _, annotations in train_data:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])\n",
    "    for doc, annotations in nlp.pipe(\n",
    "        [text for text, annotations in train_data],\n",
    "        disable=[\"tagger\", \"parser\"]\n",
    "    ):\n",
    "        nlp.update([doc], [annotations], drop=0.5)\n",
    "\n",
    "# Save the model to disk\n",
    "nlp.to_disk(\"ner_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8f5659b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'allennlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [65]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mallennlp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mallennlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpredictors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpredictor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Predictor\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mallennlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_readers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspan_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bio_tags_to_spans\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'allennlp'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import allennlp\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "from allennlp.data.dataset_readers.dataset_utils.span_utils import bio_tags_to_spans\n",
    "\n",
    "# Define the labels\n",
    "labels = [\"Disease\", \"Drug\"]\n",
    "\n",
    "import os\n",
    "os.chdir('C:/Users/p_adi/OneDrive/Desktop')\n",
    "# Read the dataframe\n",
    "df = pd.read_csv(\"output_updated.csv\")\n",
    "\n",
    "# Create the training examples\n",
    "train_data = []\n",
    "for i in range(len(df)):\n",
    "    text = df.iloc[i][\"word\"]\n",
    "    label = df.iloc[i][\"label\"]\n",
    "    train_data.append({\"sentence\":text, \"tags\":label})\n",
    "    \n",
    "# Save the data to a JSON file\n",
    "with open(\"train.json\", \"w\") as f:\n",
    "    json.dump(train_data, f)\n",
    "\n",
    "# Use the AllenNLP CRF Tagger model\n",
    "predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/crf-tagger-2020.02.10.tar.gz\")\n",
    "\n",
    "# Train the model on the data\n",
    "predictor.train_on_dataset(train_data)\n",
    "\n",
    "# Use the trained model to make predictions\n",
    "output = predictor.predict(text)\n",
    "\n",
    "# Extract the entities from the output\n",
    "entities = bio_tags_to_spans(output[\"tags\"])\n",
    "\n",
    "# Print the entities\n",
    "print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcde24ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install allennlp==0.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e77049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
